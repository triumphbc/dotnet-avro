{"version":3,"sources":["webpack:///./src/pages/internals/schema-compatibility.js"],"names":["title","to","id","language"],"mappings":"8FAAA,4FAQMA,EAAQ,uBAEC,4BACb,oCACE,kBAAC,SAAD,KACE,+BAAQA,IAGV,4BAAKA,GACL,iTAEA,oEACA,0CAAe,kBAAC,IAAD,CAAcC,GAAG,mDAAjB,mBAAf,aAA0H,kBAAC,IAAD,CAAcA,GAAG,oEAAjB,wBAA1H,4FACA,4BACE,4BAAI,sCAAJ,kCACA,4BAAI,0CAAJ,QAA8B,qDAA9B,0GACA,4BAAI,yCAAJ,QAA6B,oDAA7B,2GACA,4BAAI,sCAAJ,QAA0B,iDAA1B,wDAEF,6DAAkC,0CAAlC,+OAAmS,kBAAC,IAAD,CAAiBC,GAAG,oDAApB,sBAAnS,QAAmZ,kBAAC,IAAD,CAAiBA,GAAG,sDAApB,wBAAnZ,QAAugB,kBAAC,IAAD,CAAMD,GAAG,sBAAT,+BAAvgB,MAEA,qEACA,qKACA,4BACE,4BAAI,uDAAJ,gKAAqM,gDAArM,OAAoO,kDAApO,yGAAuW,kBAAC,IAAD,CAAcA,GAAG,gGAAjB,SAAvW,uEACA,4BAAI,4DAAJ,sEAEF,kHAAuF,kBAAC,IAAD,CAAiBC,GAAG,uCAApB,YAAvF,QAAgL,kBAAC,IAAD,CAAiBA,GAAG,uCAApB,YAAhL,oHACA,kBAAC,IAAD,CAAWC,SAAS,UAApB,uxCA+CA,kdACA,kBAAC,IAAD,CAAWA,SAAS,UAApB","file":"component---src-pages-internals-schema-compatibility-js-b8f266a73ab47ae82a25.js","sourcesContent":["import { Link } from 'gatsby'\nimport React from 'react'\nimport { Helmet } from 'react-helmet'\n\nimport Highlight from '../../components/code/highlight'\nimport DotnetReference from '../../components/references/dotnet'\nimport ExternalLink from '../../components/site/external-link'\n\nconst title = 'Schema compatibility'\n\nexport default () =>\n  <>\n    <Helmet>\n      <title>{title}</title>\n    </Helmet>\n\n    <h1>{title}</h1>\n    <p>Avro schemas are often designed with future evolution in mind: When a schema is updated, it’s generally preferable that consumers should be able to continue handling messages. One high-level goal of Chr.Avro is to facilitate flexible and predictable schema update processes.</p>\n\n    <h2>Schema Registry compatibility levels</h2>\n    <p>Confluent’s <ExternalLink to='https://github.com/confluentinc/schema-registry'>Schema Registry</ExternalLink> enforces <ExternalLink to='https://docs.confluent.io/current/schema-registry/docs/avro.html'>compatibility checks</ExternalLink> for schemas that it manages. Schema owners can choose one of these compatibility types:</p>\n    <ul>\n      <li><code>NONE</code>: No compatibility guaranteed.</li>\n      <li><code>BACKWARD</code> and <code>BACKWARD_TRANSITIVE</code>: A reader using a newer schema version must be able to read data written using older schema versions.</li>\n      <li><code>FORWARD</code> and <code>FORWARD_TRANSITIVE</code>: A reader using an older schema version must be able to read data written using newer schema versions.</li>\n      <li><code>FULL</code> and <code>FULL_TRANSITIVE</code>: Backward and forward compatibility is guaranteed.</li>\n    </ul>\n    <p>If a subject is configured for <code>BACKWARD</code> compatibility (the default), each new version can only make backward-compatible changes. These compatibility types are only used by the registry, not by Chr.Avro or other Avro libraries. (Chr.Avro only concerns itself with whether the <DotnetReference id='T:Chr.Avro.Serialization.BinarySerializerBuilder'>serializer builder</DotnetReference> and <DotnetReference id='T:Chr.Avro.Serialization.BinaryDeserializerBuilder'>deserializer builder</DotnetReference> can <Link to='/internals/mapping'>map a schema to a .NET type</Link>.)</p>\n\n    <h2>Producer and consumer implementations</h2>\n    <p>When building producers and consumers with Chr.Avro.Confluent, there are two ways to configure Avro serialization and deserialization:</p>\n    <ul>\n      <li><strong>On the fly (async):</strong> Schemas are retrieved from the Schema Registry as needed. When producing, the serializer will derive a subject name from the topic being produced to (e.g., <code>topic_name-key</code> or <code>topic_name-value</code>) and optionally register a matching schema. When consuming, the deserializer will look up the schema <ExternalLink to='https://docs.confluent.io/current/schema-registry/docs/serializer-formatter.html#wire-format'>by ID</ExternalLink>. Serializers and deserializers are not bound to a specific schema.</li>\n      <li><strong>Bound at startup (sync):</strong> Serializers and deserializers are created for a specific schema.</li>\n    </ul>\n    <p>Chr.Avro.Confluent provides some extension methods to configure the Confluent.Kafka <DotnetReference id='T:Confluent.Kafka.ProducerBuilder`2'>producer</DotnetReference> and <DotnetReference id='T:Confluent.Kafka.ConsumerBuilder`2'>consumer</DotnetReference> builders for Avro serialization. In the example below, a producer is created with serializers bound at startup:</p>\n    <Highlight language='csharp'>{`using Chr.Avro.Confluent;\nusing Confluent.Kafka;\nusing Confluent.SchemaRegistry;\nusing System;\nusing System.Threading.Tasks;\n\nnamespace Chr.Avro.Examples.ConfluentProducer\n{\n    public class Person\n    {\n        public Guid Id { get; set; }\n\n        public string Name { get; set; }\n    }\n\n    public class Program\n    {\n        public static async Task Main(string[] args)\n        {\n            var producerConfig = new ProducerConfig\n            {\n                BootstrapServers = \"broker1:9092,broker2:9092\"\n            };\n\n            var registryConfig = new SchemaRegistryConfig\n            {\n                SchemaRegistryUrl = \"http://registry:8081\"\n            };\n\n            var builder = new ProducerBuilder<Guid, Person>(producerConfig);\n\n            using (var registry = new CachedSchemaRegistryClient(registryConfig))\n            {\n                await Task.WhenAll(\n                    builder.SetAvroKeySerializer(registry, \"person-key\", registerAutomatically: true),\n                    builder.SetAvroValueSerializer(registry, \"person-value\", registerAutomatically: true)\n                );\n            }\n\n            using (var producer = builder.Build())\n            {\n                // produce\n            }\n        }\n    }\n}\n`}</Highlight>\n    <p>Binding at startup is generally good practice for producers. If a type mapping exception is thrown, it will be thrown before the producer is created and any messages are produced. For consumers, on the other hand, it’s generally better to resolve schemas on the fly—the deserializer will be able to handle updates to the schema as long as the type can be mapped. The following example demonstrates how to configure async deserializers:</p>\n    <Highlight language='csharp'>{`using Chr.Avro.Confluent;\nusing Confluent.Kafka;\nusing Confluent.SchemaRegistry;\nusing System;\nusing System.Threading.Tasks;\n\nnamespace Chr.Avro.Examples.ConfluentConsumer\n{\n    public class Person\n    {\n        public Guid Id { get; set; }\n\n        public string Name { get; set; }\n    }\n\n    public class Program\n    {\n        public static void Main(string[] args)\n        {\n            var consumerConfig = new ConsumerConfig\n            {\n                BootstrapServers = \"broker1:9092,broker2:9092\",\n                GroupId = \"example_group\"\n            };\n\n            var registryConfig = new SchemaRegistryConfig\n            {\n                SchemaRegistryUrl = \"http://registry:8081\"\n            };\n\n            var builder = new ConsumerBuilder<Guid, Person>(consumerConfig);\n\n            using (var registry = new CachedSchemaRegistryClient(registryConfig))\n            {\n                builder.SetAvroKeyDeserializer(registry);\n                builder.SetAvroValueDeserializer(registry);\n\n                using (var consumer = builder.Build())\n                {\n                    // consume\n                }\n            }\n        }\n    }\n}\n`}</Highlight>\n  </>\n"],"sourceRoot":""}